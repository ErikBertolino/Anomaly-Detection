{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker version: 2.25.1\n",
      "Artifacts will be written to s3://novelty-detection-gan-grad-sagemaker/Hyper\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "print(f'sagemaker version: {sagemaker.__version__}')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "boto_session = sagemaker_session.boto_session\n",
    "sagemaker_client = boto_session.client('sagemaker')\n",
    "\n",
    "BUCKET = 'novelty-detection-gan-grad-sagemaker'\n",
    "PREFIX = 'Hyper'\n",
    "#LOCAL_DATA_DIRECTORY = f'/Users/ccaloian/Temp/dl-pytorch/data/{PREFIX}/novelty-detection'\n",
    "\n",
    "print(f\"Artifacts will be written to s3://{BUCKET}/{PREFIX}\")\n",
    "\n",
    "# SageMaker Studio\n",
    "role = get_execution_role()\n",
    "\n",
    "# Local\n",
    "#role = \"arn:aws:iam::501545181352:role/service-role/AmazonSageMaker-ExecutionRole-20210120T103803\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters = {'epoch': 3 }\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='run.py',\n",
    "    source_dir='../source',\n",
    "    dependencies=['../requirements.txt'],\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version='1.6',\n",
    "    py_version='py3',\n",
    "    instance_count=1, \n",
    "    instance_type='ml.c5.2xlarge', #ml.p2.xlarge\n",
    "    output_path=f's3://{BUCKET}',\n",
    "    base_job_name=f'GANGrad-{PREFIX}',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 16:31:02 Starting - Starting the training job...\n",
      "2021-03-12 16:31:25 Starting - Launching requested ML instancesProfilerReport-1615566661: InProgress\n",
      "......\n",
      "2021-03-12 16:32:29 Starting - Preparing the instances for training.........\n",
      "2021-03-12 16:33:47 Downloading - Downloading input data...\n",
      "2021-03-12 16:34:28 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-03-12 16:34:39,857 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-03-12 16:34:39,873 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-12 16:34:39,881 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-03-12 16:34:42,914 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-03-12 16:34:43,397 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow==2.3.1\n",
      "  Downloading tensorflow-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\u001b[0m\n",
      "\n",
      "2021-03-12 16:34:48 Training - Training image download completed. Training in progress.\u001b[34mCollecting scikit-learn==0.24\n",
      "  Downloading scikit_learn-0.24.0-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting psutil==5.7.2\n",
      "  Downloading psutil-5.7.2.tar.gz (460 kB)\u001b[0m\n",
      "\u001b[34mCollecting matplotlib==3.3.2\n",
      "  Downloading matplotlib-3.3.2-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting pympler==0.9\n",
      "  Downloading Pympler-0.9.tar.gz (178 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (7.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24->-r requirements.txt (line 3)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\u001b[0m\n",
      "\u001b[34mCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (3.15.1)\u001b[0m\n",
      "\u001b[34mCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy>=1.15\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (0.35.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\u001b[0m\n",
      "\u001b[34mCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.36.1-cp36-cp36m-manylinux2014_x86_64.whl (4.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mCollecting termcolor>=1.1.0\u001b[0m\n",
      "\u001b[34m  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.27.1-py2.py3-none-any.whl (136 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (4.7.1)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.4.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: psutil, pympler, termcolor, wrapt\n",
      "  Building wheel for psutil (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for psutil (setup.py): finished with status 'done'\n",
      "  Created wheel for psutil: filename=psutil-5.7.2-cp36-cp36m-linux_x86_64.whl size=267330 sha256=674aeb9e2e38460f6315366e62af289375ac4f3707e90abe7808dce71bca0cdd\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/1f/18/c066f071b2c2b638e40889c5595bc6b61fbe45eb475d29d61d\n",
      "  Building wheel for pympler (setup.py): started\n",
      "  Building wheel for pympler (setup.py): finished with status 'done'\n",
      "  Created wheel for pympler: filename=Pympler-0.9-py3-none-any.whl size=164803 sha256=9727b565078ae82f4b70a08d5a98894200c00a29e78a918ead9f09504791c0a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/68/b3/75ea4cfff89982de5ea21f709b41ea6c913e2911160ed60108\n",
      "  Building wheel for termcolor (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=3432c6d824422f331660e4cf69b1ab8cc9a38b36cfb7cad0d6e75918448b36d6\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=66139 sha256=61fcc23bbc730cdc08768c02c8ec68d388004ff1663f8f589a9b1538fb7ba3f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\u001b[0m\n",
      "\u001b[34mSuccessfully built psutil pympler termcolor wrapt\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, gast, astunparse, tensorflow, scikit-learn, pympler, psutil, matplotlib\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\u001b[0m\n",
      "\u001b[34m    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.8.0\n",
      "    Uninstalling h5py-2.8.0:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled h5py-2.8.0\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\u001b[0m\n",
      "\u001b[34m    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.6.7\n",
      "    Uninstalling psutil-5.6.7:\n",
      "      Successfully uninstalled psutil-5.6.7\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.4\n",
      "    Uninstalling matplotlib-3.3.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled matplotlib-3.3.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.1 gast-0.3.3 google-auth-1.27.1 google-auth-oauthlib-0.4.3 grpcio-1.36.1 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.4 matplotlib-3.3.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 psutil-5.7.2 pyasn1-modules-0.2.8 pympler-0.9 requests-oauthlib-1.3.0 scikit-learn-0.24.0 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 wrapt-1.12.1\n",
      "\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:17,737 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:17,748 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:17,757 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:17,766 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epoch\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"GANGrad-Hyper-2021-03-12-16-31-01-831\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://novelty-detection-gan-grad-sagemaker/GANGrad-Hyper-2021-03-12-16-31-01-831/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epoch\":3}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://novelty-detection-gan-grad-sagemaker/GANGrad-Hyper-2021-03-12-16-31-01-831/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epoch\":3},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"GANGrad-Hyper-2021-03-12-16-31-01-831\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://novelty-detection-gan-grad-sagemaker/GANGrad-Hyper-2021-03-12-16-31-01-831/source/sourcedir.tar.gz\",\"module_name\":\"run\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epoch\",\"3\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run.py --epoch 3\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-12 16:35:35 Uploading - Uploading generated training model\n",
      "2021-03-12 16:35:35 Failed - Training job failed\n",
      "\u001b[34mInitializing Dataset...\u001b[0m\n",
      "\u001b[34mMNIST\u001b[0m\n",
      "\u001b[34mDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\u001b[0m\n",
      "\u001b[34m#015    8192/11490434 [..............................] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 4202496/11490434 [=========>....................] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 8396800/11490434 [====================>.........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01511493376/11490434 [==============================] - 1s 0us/step\u001b[0m\n",
      "\u001b[34mInlier classes are:\u001b[0m\n",
      "\u001b[34m1\u001b[0m\n",
      "\u001b[34mSplitting dataset\u001b[0m\n",
      "\u001b[34mNumber of data\u001b[0m\n",
      "\u001b[34mTraining: 1000, Test: 3000\n",
      "\u001b[0m\n",
      "\u001b[34mInformation of data\u001b[0m\n",
      "\u001b[34mShape  Height: 28, Width: 28, Channel: 1\u001b[0m\n",
      "\u001b[34mValue  Min: 0.000, Max: 255.000\u001b[0m\n",
      "\u001b[34mClass  10\u001b[0m\n",
      "\u001b[34mNormalization: True\u001b[0m\n",
      "\u001b[34m(from 0.000-255.000 to 0.000-1.000)\u001b[0m\n",
      "\u001b[34mEncoder(\n",
      "  (en_conv): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ELU(alpha=1.0)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ELU(alpha=1.0)\n",
      "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ELU(alpha=1.0)\n",
      "  )\n",
      "  (en_dense): Sequential(\n",
      "    (0): Flatten()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mDecoder(\n",
      "  (de_dense): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Linear(in_features=512, out_features=3136, bias=True)\n",
      "    (4): BatchNorm1d(3136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "  )\n",
      "  (de_conv): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ELU(alpha=1.0)\n",
      "    (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ELU(alpha=1.0)\n",
      "    (12): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ELU(alpha=1.0)\n",
      "    (15): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ELU(alpha=1.0)\n",
      "    (18): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mDiscriminator(\n",
      "  (dis_conv): ModuleList(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ELU(alpha=1.0)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ELU(alpha=1.0)\n",
      "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ELU(alpha=1.0)\n",
      "  )\n",
      "  (dis_dense): ModuleList(\n",
      "    (0): Flatten()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (5): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Sigmoid()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mThe number of parameters: 5234390\n",
      "\u001b[0m\n",
      "\u001b[34mTraining to 3 epochs (32 of minibatch size)\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.358 algo-1:90 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.548 algo-1:90 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.548 algo-1:90 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.549 algo-1:90 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.550 algo-1:90 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.550 algo-1:90 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.550 algo-1:90 INFO hook.py:550] name:en_conv.0.weight count_params:144\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.550 algo-1:90 INFO hook.py:550] name:en_conv.0.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.550 algo-1:90 INFO hook.py:550] name:en_conv.1.weight count_params:16\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.550 algo-1:90 INFO hook.py:550] name:en_conv.1.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.3.weight count_params:2304\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.3.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.4.weight count_params:16\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.4.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.7.weight count_params:4608\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.7.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.8.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.8.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.10.weight count_params:9216\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.10.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.11.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.11.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.14.weight count_params:18432\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.14.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.15.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.15.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.17.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.17.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.18.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_conv.18.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_dense.1.weight count_params:1605632\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_dense.1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.551 algo-1:90 INFO hook.py:550] name:en_dense.2.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.552 algo-1:90 INFO hook.py:550] name:en_dense.2.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.552 algo-1:90 INFO hook.py:550] name:en_dense.4.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.552 algo-1:90 INFO hook.py:550] name:en_dense.4.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.552 algo-1:90 INFO hook.py:550] name:en_dense.5.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.552 algo-1:90 INFO hook.py:550] name:en_dense.5.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.552 algo-1:90 INFO hook.py:552] Total Trainable Params: 1745328\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.552 algo-1:90 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-03-12 16:35:23.554 algo-1:90 INFO hook.py:476] Hook is writing from the hook with pid: 90\n",
      "\u001b[0m\n",
      "\u001b[34mInside ref_grad count\u001b[0m\n",
      "\u001b[34mBatch iteration is: \u001b[0m\n",
      "\u001b[34m1\u001b[0m\n",
      "\u001b[34mPercentage of RAM available memory\u001b[0m\n",
      "\u001b[34m65.11952615684683\u001b[0m\n",
      "\u001b[34mCurrent memory usage is MB\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:19.114483: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib:/opt/conda/lib:/home/.openmpi/lib/\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:19.114529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"run.py\", line 119, in <module>\n",
      "    main(FLAGS.model, FLAGS.output)\n",
      "  File \"run.py\", line 81, in main\n",
      "    Enc_weight=FLAGS.Enc_weight, Adv_weight=FLAGS.Adv_weight, Con_weight=FLAGS.Con_weight)\n",
      "  File \"/opt/ml/code/solver.py\", line 489, in training\n",
      "    print(current/10**6)\u001b[0m\n",
      "\u001b[34mNameError: name 'current' is not defined\n",
      "\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:25,304 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.6 run.py --epoch 3\"\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:19.114483: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib:/opt/conda/lib:/home/.openmpi/lib/\u001b[0m\n",
      "\u001b[34m2021-03-12 16:35:19.114529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"run.py\", line 119, in <module>\n",
      "    main(FLAGS.model, FLAGS.output)\n",
      "  File \"run.py\", line 81, in main\n",
      "    Enc_weight=FLAGS.Enc_weight, Adv_weight=FLAGS.Adv_weight, Con_weight=FLAGS.Con_weight)\n",
      "  File \"/opt/ml/code/solver.py\", line 489, in training\n",
      "    print(current/10**6)\u001b[0m\n",
      "\u001b[34mNameError: name 'current' is not defined\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job GANGrad-Hyper-2021-03-12-16-31-01-831: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run.py --epoch 3\"\n2021-03-12 16:35:19.114483: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib:/opt/conda/lib:/home/.openmpi/lib/\n2021-03-12 16:35:19.114529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTraceback (most recent call last):\n  File \"run.py\", line 119, in <module>\n    main(FLAGS.model, FLAGS.output)\n  File \"run.py\", line 81, in main\n    Enc_weight=FLAGS.Enc_weight, Adv_weight=FLAGS.Adv_weight, Con_weight=FLAGS.Con_weight)\n  File \"/opt/ml/code/solver.py\", line 489, in training\n    print(current/10**6)\nNameError: name 'current' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-28be9b2c12b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3640\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3641\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3642\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3220\u001b[0m                 ),\n\u001b[1;32m   3221\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3223\u001b[0m             )\n\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job GANGrad-Hyper-2021-03-12-16-31-01-831: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run.py --epoch 3\"\n2021-03-12 16:35:19.114483: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib:/opt/conda/lib:/home/.openmpi/lib/\n2021-03-12 16:35:19.114529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTraceback (most recent call last):\n  File \"run.py\", line 119, in <module>\n    main(FLAGS.model, FLAGS.output)\n  File \"run.py\", line 81, in main\n    Enc_weight=FLAGS.Enc_weight, Adv_weight=FLAGS.Adv_weight, Con_weight=FLAGS.Con_weight)\n  File \"/opt/ml/code/solver.py\", line 489, in training\n    print(current/10**6)\nNameError: name 'current' is not defined"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
